{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1nNNBhcmWIkrp8V9iyYlpSQ7pZhvy8uzZ","timestamp":1736562072437}],"authorship_tag":"ABX9TyOZxE8AtV/XnQOVEgv06pfZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"8lEBbNERIETr"},"outputs":[],"source":["import os\n","import torch\n","from safetensors.torch import load_file, save_file\n","from collections import defaultdict\n","from tqdm import tqdm\n","import gc\n","\n","def load_lora(lora_path):\n","    \"\"\"Load a LoRA file and organize its layers\"\"\"\n","    if lora_path.endswith('.safetensors'):\n","        lora_state_dict = load_file(lora_path)\n","    else:\n","        lora_state_dict = torch.load(lora_path, map_location='cpu')\n","\n","    # Organize LoRA keys by type\n","    lora_layers = defaultdict(dict)\n","    for key in lora_state_dict.keys():\n","        if 'lora_down' in key:\n","            base_key = key.replace('lora_down', '')\n","            lora_layers[base_key]['down'] = lora_state_dict[key]\n","        elif 'lora_up' in key:\n","            base_key = key.replace('lora_up', '')\n","            lora_layers[base_key]['up'] = lora_state_dict[key]\n","        elif 'alpha' in key:\n","            base_key = key.replace('_alpha', '')\n","            lora_layers[base_key]['alpha'] = lora_state_dict[key]\n","\n","    return lora_layers\n","\n","def merge_lora_into_model(model_path, lora_path, output_path, lora_weight=1.0):\n","    \"\"\"Merge LoRA weights into a checkpoint\"\"\"\n","    print(f\"Loading base model from {model_path}\")\n","    model = load_file(model_path)\n","\n","    print(f\"Loading LoRA from {lora_path}\")\n","    lora_layers = load_lora(lora_path)\n","\n","    print(f\"Starting merge process with weight {lora_weight}\")\n","    merged_model = {}\n","    modified_count = 0\n","\n","    # Map LoRA keys to model keys\n","    lora_to_model = {}\n","    for model_key in model.keys():\n","        # Remove common prefixes for matching\n","        clean_key = model_key.replace('model.diffusion_model.', '')\n","        clean_key = clean_key.replace('first_stage_model.', '')\n","        clean_key = clean_key.replace('model.', '')\n","\n","        for lora_key in lora_layers.keys():\n","            if clean_key in lora_key:\n","                lora_to_model[lora_key] = model_key\n","\n","    # Perform merge\n","    for key in tqdm(model.keys(), desc=\"Merging LoRA\"):\n","        if key in merged_model:\n","            continue\n","\n","        # Copy original tensor by default\n","        merged_model[key] = model[key].clone()\n","\n","        # Check if this layer has a corresponding LoRA\n","        matching_keys = [k for k, v in lora_to_model.items() if v == key]\n","        if matching_keys:\n","            lora_key = matching_keys[0]\n","            if 'down' in lora_layers[lora_key] and 'up' in lora_layers[lora_key]:\n","                down = lora_layers[lora_key]['down']\n","                up = lora_layers[lora_key]['up']\n","                alpha = lora_layers[lora_key].get('alpha', torch.tensor(1.0))\n","\n","                # Calculate scale factor\n","                if torch.is_tensor(alpha):\n","                    scale = alpha / down.shape[0]\n","                else:\n","                    scale = 1.0\n","\n","                # Compute LoRA contribution\n","                lora_contribution = torch.mm(up.float(), down.float()) * scale * lora_weight\n","\n","                # Add to base weights\n","                if lora_contribution.shape == merged_model[key].shape:\n","                    merged_model[key] = merged_model[key].float() + lora_contribution\n","                    modified_count += 1\n","\n","        # Convert back to half precision\n","        merged_model[key] = merged_model[key].half()\n","\n","        # Save incrementally\n","        if len(merged_model) >= 1000:\n","            save_file(merged_model, output_path)\n","            merged_model = {}\n","            gc.collect()\n","            torch.cuda.empty_cache()\n","\n","    # Save remaining tensors\n","    if merged_model:\n","        save_file(merged_model, output_path)\n","\n","    print(f\"\\nMerge completed:\")\n","    print(f\"Modified {modified_count} layers with LoRA weights\")\n","    print(f\"Output saved to {output_path}\")\n","\n","    # Validate final model\n","    print(\"\\nValidating merged model...\")\n","    final_model = load_file(output_path)\n","    print(f\"Final model contains {len(final_model.keys())} keys\")\n","\n","if __name__ == \"__main__\":\n","    # Updated paths for your directory structure\n","    checkpoint_dir = \"/content/stable-diffusion-webui-reForge/models/Stable-diffusion\"\n","    lora_dir = \"/content/stable-diffusion-webui-reForge/models/Lora\"\n","\n","    # Example paths - replace with your actual filenames\n","    base_model_path = os.path.join(checkpoint_dir, \"your_base_model.safetensors\")  # Replace with your base model filename\n","    lora_path = os.path.join(lora_dir, \"your_lora.safetensors\")  # Replace with your LoRA filename\n","    output_path = os.path.join(checkpoint_dir, \"merged_with_lora.safetensors\")\n","\n","    # Merge settings\n","    lora_weight = 0.75  # Adjust this value to control LoRA influence (0.0 to 1.0)\n","\n","    # Perform merge\n","    merge_lora_into_model(\n","        model_path=base_model_path,\n","        lora_path=lora_path,\n","        output_path=output_path,\n","        lora_weight=lora_weight\n","    )"]}]}